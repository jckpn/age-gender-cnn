{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook is useful as we can keep datasets in memory between training runs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and processing 1000 files from C:\\Users\\jckpn\\Documents\\YEAR 3 PROJECT\\implementation\\source\\other\\imdb_crop into `C:\\Users\\jckpn\\Documents\\YEAR 3 PROJECT\\implementation\\source\\other\\imdb_crop_processed`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:04<1:09:19,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalising - 10 entries per class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000 items successfully prepared\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and processing 1000 files from C:\\Users\\jckpn\\Documents\\YEAR 3 PROJECT\\implementation\\source\\datasets\\training\\raw_imdbwiki_crop\\wiki_crop into `C:\\Users\\jckpn\\Documents\\YEAR 3 PROJECT\\implementation\\source\\datasets\\training\\raw_imdbwiki_crop\\wiki_crop_processed`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [00:00<02:27,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalising - 8 entries per class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000 items successfully prepared\n",
      "\n",
      "Reading and processing 1000 files from C:\\Users\\jckpn\\Documents\\YEAR 3 PROJECT\\implementation\\source\\utkface_itw into `C:\\Users\\jckpn\\Documents\\YEAR 3 PROJECT\\implementation\\source\\utkface_itw_processed`...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1000 [00:00<02:48,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalising - 9 entries per class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000 items successfully prepared\n",
      "\n",
      "Split dataset into 2400 training and 600 validation examples\n"
     ]
    }
   ],
   "source": [
    "# Don't cache other files https://stackoverflow.com/a/57245926\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train import train_model\n",
    "from torch.utils.data import random_split, ConcatDataset\n",
    "from face_dataset import *\n",
    "from label_funcs import *\n",
    "import preprocessor\n",
    "from ds_transforms import *\n",
    "\n",
    "# Define dataset parameters\n",
    "ds_size = 1000\n",
    "equalise = False\n",
    "processor = preprocessor.processor(crop='face')\n",
    "transform = alexnet_transform(84)\n",
    "equalise = True\n",
    "augment = True\n",
    "print_errors = False\n",
    "\n",
    "# changes for proper dev:\n",
    "# save processed datasets to local storage\n",
    "# patience = 10\n",
    "# max_epochs = 100\n",
    "\n",
    "# adience_gender_ds = FastDataset(\n",
    "#     'C:\\\\Users\\\\jckpn\\\\Documents\\\\YEAR 3 PROJECT\\\\implementation\\\\source\\\\datasets\\\\training\\\\adience',\n",
    "#     label_func=age_label_all, transform=transform, processor=processor,\n",
    "#     ds_size=ds_size, print_errors=print_errors, equalise=equalise,\n",
    "#     augment=augment)\n",
    "\n",
    "imdb_gender_ds = StorageDataset(\n",
    "    'C:\\\\Users\\\\jckpn\\\\Documents\\\\YEAR 3 PROJECT\\\\implementation\\\\source\\\\other\\\\imdb_crop',\n",
    "    label_func=age_label_all, transform=transform, processor=processor,\n",
    "    ds_size=ds_size, print_errors=print_errors, equalise=equalise,\n",
    "    augment=augment)\n",
    "\n",
    "wiki_gender_ds = StorageDataset(\n",
    "    'C:\\\\Users\\\\jckpn\\\\Documents\\\\YEAR 3 PROJECT\\\\implementation\\\\source\\\\datasets\\\\training\\\\raw_imdbwiki_crop\\\\wiki_crop',\n",
    "    label_func=age_label_all, transform=transform, processor=processor,\n",
    "    ds_size=ds_size, print_errors=print_errors, equalise=equalise,\n",
    "    augment=augment)\n",
    "\n",
    "utkface_gender_ds = StorageDataset(\n",
    "    'C:\\\\Users\\\\jckpn\\\\Documents\\\\YEAR 3 PROJECT\\\\implementation\\\\source\\\\utkface_itw',\n",
    "    label_func=age_label_all_utk, transform=transform, processor=processor,\n",
    "    ds_size=ds_size, print_errors=print_errors, equalise=equalise,\n",
    "    augment=augment)\n",
    "    \n",
    "train_val_set = ConcatDataset([wiki_gender_ds, imdb_gender_ds, utkface_gender_ds])\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "val_split_ratio = 0.2\n",
    "val_size = int(val_split_ratio * len(train_val_set))\n",
    "train_size = len(train_val_set) - val_size\n",
    "train_set, val_set = random_split(train_val_set, [train_size, val_size])\n",
    "print(f'Split dataset into {len(train_set)} training and {len(val_set)} validation examples')\n",
    "\n",
    "test_set = val_set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train desired models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\jckpn/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING MODEL AlexNet-1_2604-1941.pt WITH PARAMS:\n",
      " - Architecture: AlexNet\n",
      " - Learning rate: 0.0001\n",
      " - Optimizer: Adam\n",
      " - Loss function: MSELoss()\n",
      " - Other notes: None\n",
      "\n",
      "+---------------+---------------+---------------+---------------+---------------+\n",
      "|         EPOCH | EXAMPLES SEEN |    TRAIN LOSS |      VAL LOSS |  ELAPSED TIME |\n",
      "+---------------+---------------+---------------+---------------+---------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|             1 |          2400 |         8.887 |         6.426 |          2:06 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|             2 |          4800 |         6.278 |         6.277 |          3:27 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|             3 |          7200 |         4.939 |         5.648 |          4:50 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| *           4 |          9600 |          4.23 |         5.843 |          6:11 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|             5 |         12000 |         3.947 |         5.089 |          7:38 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| *           6 |         14400 |         2.761 |         5.327 |          9:04 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|             7 |         16800 |         2.757 |         4.997 |         10:30 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| *           8 |         19200 |         2.133 |         5.369 |         17:54 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|             9 |         21600 |         2.147 |         4.937 |         19:35 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| *          10 |         24000 |         1.675 |         5.072 |         21:03 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| **         11 |         26400 |         1.686 |         5.002 |         22:31 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ***        12 |         28800 |         1.637 |         4.949 |         24:09 |\n",
      "+---------------+---------------+---------------+---------------+---------------+\n",
      "\n",
      "Halting training - 3 epochs without improvement\n",
      "\n",
      "Training took 24m 09s (120.0s per epoch)\n",
      "\n",
      "Best model from session saved to '../models\\AlexNet-1_2604-1941.pt'\n",
      "\n",
      "Testing model accuracy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.1773\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 145] The directory is not empty: 'C:\\\\Users\\\\jckpn\\\\Documents\\\\YEAR 3 PROJECT\\\\implementation\\\\source\\\\other\\\\imdb_crop_processed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 18\u001b[0m\n\u001b[0;32m      6\u001b[0m model \u001b[39m=\u001b[39m train_model(\n\u001b[0;32m      7\u001b[0m     model\u001b[39m=\u001b[39mAlexNet(\u001b[39m1\u001b[39m), \n\u001b[0;32m      8\u001b[0m     train_set\u001b[39m=\u001b[39mtrain_set,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     patience\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[0;32m     14\u001b[0m     model_save_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m../models\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m tests\u001b[39m.\u001b[39mmae(model, test_set, print_results\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 18\u001b[0m imdb_gender_ds\u001b[39m.\u001b[39;49mdelete()\n\u001b[0;32m     19\u001b[0m wiki_gender_ds\u001b[39m.\u001b[39mdelete()\n\u001b[0;32m     20\u001b[0m utkface_gender_ds\u001b[39m.\u001b[39mdelete()\n",
      "File \u001b[1;32mc:\\Users\\jckpn\\Documents\\YEAR 3 PROJECT\\implementation\\source\\age-gender-cnn\\face_dataset.py:264\u001b[0m, in \u001b[0;36mdelete\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         image = torch.load(f'{self.write_dir}/{filename}')\n\u001b[0;32m    261\u001b[0m         return image, entry['label']\n\u001b[0;32m    263\u001b[0m     def delete(self):\n\u001b[1;32m--> 264\u001b[0m         # Delete processed images from disk\n\u001b[0;32m    265\u001b[0m         shutil.rmtree(self.write_dir)\n\u001b[0;32m    267\u001b[0m # redundant, remove this later\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 145] The directory is not empty: 'C:\\\\Users\\\\jckpn\\\\Documents\\\\YEAR 3 PROJECT\\\\implementation\\\\source\\\\other\\\\imdb_crop_processed'"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.nn import CrossEntropyLoss, MSELoss  # classif. -> CEL, regress. -> MSE\n",
    "from torch.optim import Adam, SGD\n",
    "from networks import *\n",
    "import tests\n",
    "\n",
    "model = train_model(\n",
    "    model=AlexNet(1), \n",
    "    train_set=train_set,\n",
    "    val_set=val_set,\n",
    "    loss_fn=MSELoss(),\n",
    "    optim_fn=Adam,\n",
    "    learning_rate=0.0001,\n",
    "    patience=3,\n",
    "    model_save_dir='../models')\n",
    "\n",
    "tests.mae(model, test_set, print_results=True)\n",
    "\n",
    "imdb_gender_ds.delete()\n",
    "wiki_gender_ds.delete()\n",
    "utkface_gender_ds.delete()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
