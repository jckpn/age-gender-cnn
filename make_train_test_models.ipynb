{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook is useful as we can keep datasets in memory between training runs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and processing 6667 files from C:\\Users\\jckpn\\Documents\\YEAR 3 PROJECT\\implementation\\source\\other\\imdb_crop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6667 items successfully prepared\n",
      "\n",
      "Reading and processing 6667 files from C:\\Users\\jckpn\\Documents\\YEAR 3 PROJECT\\implementation\\source\\datasets\\training\\raw_imdbwiki_crop\\wiki_crop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6667 items successfully prepared\n",
      "\n",
      "Reading and processing 6667 files from C:\\Users\\jckpn\\Documents\\YEAR 3 PROJECT\\implementation\\source\\utkface_itw...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6667 items successfully prepared\n",
      "\n",
      "Split dataset into 16001 training and 4000 validation examples\n"
     ]
    }
   ],
   "source": [
    "# Don't cache my other files https://stackoverflow.com/a/57245926\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train import train_model\n",
    "from torch.utils.data import random_split, ConcatDataset\n",
    "from face_dataset import *\n",
    "from label_funcs import *\n",
    "import preprocessor\n",
    "from ds_transforms import *\n",
    "\n",
    "# Define dataset parameters\n",
    "ds_size = 6667\n",
    "equalise = False\n",
    "processor = preprocessor.processor(w=84, h=84)\n",
    "transform = lenet_transform(size=84)\n",
    "classes = 100\n",
    "equlalise = False\n",
    "augment = False\n",
    "print_errors = False\n",
    "\n",
    "# adience_gender_ds = SlowDataset(\n",
    "#     'C:\\\\Users\\\\jckpn\\\\Documents\\\\YEAR 3 PROJECT\\\\implementation\\\\source\\\\datasets\\\\training\\\\adience',\n",
    "#     age_label_all, transform, processor, ds_size=50, equalise=True, classes=2, augment=False, print_errors=False)\n",
    "\n",
    "imdb_gender_ds = FastDataset(\n",
    "    'C:\\\\Users\\\\jckpn\\\\Documents\\\\YEAR 3 PROJECT\\\\implementation\\\\source\\\\other\\\\imdb_crop',\n",
    "    age_label_all, transform, processor, ds_size, print_errors, equalise=equalise, augment=augment, classes=classes)\n",
    "\n",
    "wiki_gender_ds = FastDataset(\n",
    "    'C:\\\\Users\\\\jckpn\\\\Documents\\\\YEAR 3 PROJECT\\\\implementation\\\\source\\\\datasets\\\\training\\\\raw_imdbwiki_crop\\\\wiki_crop',\n",
    "    age_label_all, transform, processor, ds_size, print_errors, equalise=equalise, augment=augment, classes=classes)\n",
    "\n",
    "utkface_gender_ds = FastDataset(\n",
    "    'C:\\\\Users\\\\jckpn\\\\Documents\\\\YEAR 3 PROJECT\\\\implementation\\\\source\\\\utkface_itw',\n",
    "    age_label_all, transform, processor, ds_size, print_errors, equalise=equalise, augment=augment, classes=classes)\n",
    "    \n",
    "train_val_set = ConcatDataset([wiki_gender_ds, imdb_gender_ds, utkface_gender_ds])\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "val_split_ratio = 0.2\n",
    "val_size = int(val_split_ratio * len(train_val_set))\n",
    "train_size = len(train_val_set) - val_size\n",
    "train_set, val_set = random_split(train_val_set, [train_size, val_size])\n",
    "print(f'Split dataset into {len(train_set)} training and {len(val_set)} validation examples')\n",
    "\n",
    "test_set = val_set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train desired models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING MODEL LeNet-1_2604-1346.pt WITH PARAMS:\n",
      " - Architecture: LeNet\n",
      " - Learning rate: 0.001\n",
      " - Optimizer: SGD\n",
      " - Loss function: MSELoss()\n",
      " - Other notes: None\n",
      "\n",
      "+---------------+---------------+---------------+---------------+---------------+\n",
      "|         EPOCH | EXAMPLES SEEN |    TRAIN LOSS |      VAL LOSS |  ELAPSED TIME |\n",
      "+---------------+---------------+---------------+---------------+---------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|             1 |         16001 |        19.926 |        12.810 |          0:41 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| *           2 |         32002 |        12.980 |        13.594 |          1:22 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| **          3 |         48003 |        13.040 |        13.413 |          2:07 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  16%|█▋        | 82/501 [00:06<00:31, 13.13it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.nn import CrossEntropyLoss, MSELoss  # classif. -> CEL, regress. -> MSE\n",
    "from torch.optim import Adam, SGD\n",
    "from networks import *\n",
    "import tests\n",
    "\n",
    "model = train_model(\n",
    "    model=LeNet(1), \n",
    "    train_set=train_set,\n",
    "    val_set=val_set,\n",
    "    loss_fn=MSELoss(),\n",
    "    optim_fn=SGD,\n",
    "    learning_rate=0.001,\n",
    "    model_save_dir='../models')\n",
    "\n",
    "tests.mae(model, test_set, print_results=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
